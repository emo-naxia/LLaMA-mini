import torch
from models.model import LLaMAModel
from models.model_config import ModelConfig
from tokenizer import Tokenizer
from config import MODEL_PATH, VOCAB_PATH, TOKENIZER_PATH

# ==== è‡ªåŠ¨é€‚é…è®¾å¤‡ ====
DEVICE = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"

# ==== åˆå§‹åŒ– tokenizer ====
tokenizer = Tokenizer(model_path=TOKENIZER_PATH, bos_token="[BOS]", eos_token="[EOS]", unk_token="[UNK]")

# ==== åˆå§‹åŒ–æ¨¡å‹ ====
config = ModelConfig()
model = LLaMAModel(config)
model.load_state_dict(torch.load(MODEL_PATH, map_location="cpu"))
model.to(DEVICE)  # âœ… å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
model.eval()

# ==== ç®€å•æ¨ç†å‡½æ•° ====
def generate_text(prompt, max_length=50):
    input_ids = tokenizer.encode(prompt)
    input_ids = torch.tensor(input_ids).unsqueeze(0).to(DEVICE)  # âœ… æ”¾å…¥è®¾å¤‡
    with torch.no_grad():
        for _ in range(max_length):
            logits = model(input_ids)
            next_token = torch.argmax(logits[:, -1, :], dim=-1)
            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)
            if next_token.item() == tokenizer.eos_id:
                break
    return tokenizer.decode(input_ids[0].tolist())

# ==== äº¤äº’ ====
prompt = input("ğŸ’¬ è¯·è¾“å…¥ä¸€å¥å¼€å¤´ï¼š")
result = generate_text(prompt)
print("ğŸ§  æ¨¡å‹ç”Ÿæˆç»“æœï¼š", result)